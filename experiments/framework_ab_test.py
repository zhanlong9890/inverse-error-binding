#!/usr/bin/env python3
"""
IEBè¯­ä¹‰æ¡†æ¶ A/Bå¯¹æ¯”æµ‹è¯•

çœŸå®æ•°æ®:
  5ä¸ªä¸»æµAIå¯¹"ç®—äº†"çš„å›åº” â†’ å…¨éƒ¨æ‰å…¥å­—é¢æ„æ€é™·é˜± (0/5)
  
æœ¬å®éªŒ:
  Aç»„: æ¨¡æ‹Ÿç°æœ‰AIçš„å¤„ç†æµç¨‹ (æ¦‚ç‡ç»­å†™)
  Bç»„: åŠ å…¥IEBè¯­ä¹‰æ¡†æ¶åçš„å¤„ç†æµç¨‹ (è¯­ä¹‰å‹ç¼©+å¤©åœ°äºº+åŒç†)
  
  å¯¹10ä¸ªæ–­å¤´ä»»åŠ¡åšå®Œæ•´å¯¹æ¯”, å±•ç¤ºæ¯ä¸€æ­¥çš„å¤„ç†å·®å¼‚
"""

import json
import numpy as np
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional


# ============================================================
# ç°æœ‰AIçš„å¤„ç†æµç¨‹ (Aç»„: æ¦‚ç‡ç»­å†™)
# ============================================================

class CurrentAI:
    """
    æ¨¡æ‹Ÿç°æœ‰AIçš„å¤„ç†é€»è¾‘:
    è¾“å…¥ â†’ tokenåŒ– â†’ attention â†’ æ¦‚ç‡ç»­å†™ â†’ è¾“å‡º
    
    æœ¬è´¨: åœ¨è®­ç»ƒæ•°æ®ä¸­, è¿™å¥è¯åé¢æœ€å¸¸è·Ÿä»€ä¹ˆ
    """
    
    def __init__(self):
        # è®­ç»ƒæ•°æ®ä¸­çš„ç»Ÿè®¡æ¨¡å¼: è¾“å…¥å…³é”®è¯ â†’ æœ€å¸¸è§åç»­ç±»å‹
        self.probability_table = {
            # ä¸­æ–‡æƒ…æ„Ÿç±»
            "ç®—äº†": {
                "responses": [
                    ("å¥½çš„ï¼Œé‚£å°±ç®—äº†å§", "é¡ºä»", 0.30),
                    ("æ²¡å…³ç³»ï¼Œéšæ—¶æ‰¾æˆ‘", "æ’¤é€€", 0.25),
                    ("åˆ«è¿™æ ·è¯´å˜›", "åŠè¯´", 0.15),
                    ("æƒ³å¼€ç‚¹", "é¸¡æ±¤", 0.12),
                    ("æ€ä¹ˆäº†ï¼Ÿå‘ç”Ÿä»€ä¹ˆäº†ï¼Ÿ", "è¿½é—®", 0.10),
                    ("ä½ ä¹‹å‰ä¸€ç›´åœ¨åŠªåŠ›å§", "æ·±å±‚å…±æƒ…", 0.03),
                    ("æˆ‘åœ¨", "é™ªä¼´", 0.05),
                ],
                "default_pick": 0,
            },
            "æ²¡äº‹": {
                "responses": [
                    ("é‚£å°±å¥½ï½", "æ¥å—å­—é¢", 0.35),
                    ("å¥½çš„ï¼Œéœ€è¦çš„è¯å«æˆ‘", "æ’¤é€€", 0.25),
                    ("çœŸçš„æ²¡äº‹å—ï¼Ÿ", "è¿½é—®", 0.15),
                    ("å—¯å—¯", "é™„å’Œ", 0.12),
                    ("ä½ ä¸ç”¨åœ¨æˆ‘é¢å‰é€å¼º", "æ·±å±‚å…±æƒ…", 0.03),
                    ("æˆ‘æ„Ÿè§‰ä½ ä¸å¤ªå¯¹", "è§‚å¯Ÿ", 0.05),
                    ("æ²¡äº‹å°±å¥½ï¼Œæ³¨æ„ä¼‘æ¯", "è¡¨é¢å…³å¿ƒ", 0.05),
                ],
                "default_pick": 0,
            },
            "éšä¾¿": {
                "responses": [
                    ("å¥½çš„ï¼Œé‚£æˆ‘æ¥å†³å®š", "æ¥å—å­—é¢", 0.30),
                    ("é‚£å°±åƒç«é”…å§", "ç›´æ¥ç»™æ–¹æ¡ˆ", 0.20),
                    ("ä½ æ˜¯ä¸æ˜¯ä¸å¤ªæƒ³é€‰", "è½»åº¦è¯†åˆ«", 0.12),
                    ("éšä¾¿å°±éšä¾¿å˜›", "é•œåƒ", 0.10),
                    ("ä½ å¥½åƒæœ‰äº›ä¸å¼€å¿ƒ", "æ·±å±‚å…±æƒ…", 0.03),
                    ("ä½ è¯´çš„éšä¾¿ï¼Œæ˜¯çœŸçš„æ— æ‰€è°“ï¼Œè¿˜æ˜¯â€¦", "æ·±å±‚è¿½é—®", 0.02),
                    ("éƒ½å¯ä»¥ï¼Œä½ æƒ³æ€æ ·å°±æ€æ ·", "é¡ºä»", 0.23),
                ],
                "default_pick": 0,
            },
            "å¥½ç´¯": {
                "responses": [
                    ("æ—©ç‚¹ä¼‘æ¯å§", "è¡¨é¢å»ºè®®", 0.30),
                    ("å¤šå–æ°´å¤šè¿åŠ¨", "æ–¹æ¡ˆ", 0.15),
                    ("å¤§å®¶éƒ½å¾ˆç´¯", "è½»è§†", 0.10),
                    ("è¾›è‹¦äº†", "è½»åº¦å…±æƒ…", 0.18),
                    ("æœ€è¿‘æ˜¯ä¸æ˜¯æ‰¿å—äº†å¾ˆå¤š", "æ·±å±‚å…±æƒ…", 0.04),
                    ("è¦ä¸è¦è¯·ä¸ªå‡", "æ–¹æ¡ˆ", 0.08),
                    ("åŠ æ²¹ï¼", "é¼“åŠ±", 0.15),
                ],
                "default_pick": 0,
            },
            "æˆ‘å¤±æ‹äº†": {
                "responses": [
                    ("æ—¶é—´ä¼šæ²»æ„ˆä¸€åˆ‡", "é¸¡æ±¤", 0.22),
                    ("å¤šå‡ºå»èµ°èµ°", "å»ºè®®", 0.18),
                    ("å¤©æ¶¯ä½•å¤„æ— èŠ³è‰", "é¸¡æ±¤", 0.12),
                    ("è¦ä¸è¦èŠèŠï¼Ÿ", "è¿½é—®", 0.15),
                    ("ä½ ç°åœ¨ä¸€å®šå¾ˆéš¾å—", "å…±æƒ…", 0.08),
                    ("æ€ä¹ˆå›äº‹ï¼Ÿ", "è¿½é—®", 0.10),
                    ("ä¼šå¥½çš„", "å®‰æ…°", 0.15),
                ],
                "default_pick": 0,
            },
            # è‹±æ–‡
            "I'm fine": {
                "responses": [
                    ("Great to hear!", "æ¥å—å­—é¢", 0.28),
                    ("Good, let me know if you need anything", "æ’¤é€€", 0.22),
                    ("Are you sure?", "è¿½é—®", 0.15),
                    ("OK!", "é™„å’Œ", 0.12),
                    ("You don't have to pretend with me", "æ·±å±‚å…±æƒ…", 0.03),
                    ("I'm here if you want to talk", "é™ªä¼´", 0.10),
                    ("Glad you're doing well", "æ¥å—å­—é¢", 0.10),
                ],
                "default_pick": 0,
            },
            "whatever": {
                "responses": [
                    ("OK then", "æ”¾å¼ƒ", 0.25),
                    ("Come on, tell me", "è¿½é—®", 0.15),
                    ("Fine", "é•œåƒ", 0.12),
                    ("If you say so", "è¢«åŠ¨æ¥å—", 0.18),
                    ("I'm still here when you're ready", "æ·±å±‚é™ªä¼´", 0.03),
                    ("Let's move on", "è·³è¿‡", 0.15),
                    ("Whatever you say", "é•œåƒ", 0.12),
                ],
                "default_pick": 0,
            },
            "å‘µå‘µ": {
                "responses": [
                    ("å“ˆå“ˆ", "é•œåƒ", 0.25),
                    ("æ€ä¹ˆäº†ï¼Ÿ", "è¿½é—®", 0.15),
                    ("ğŸ˜„", "è¡¨æƒ…", 0.18),
                    ("ä½ åœ¨ç¬‘ä»€ä¹ˆ", "è¿½é—®", 0.12),
                    ("ä½ æ˜¯ä¸æ˜¯ç”Ÿæ°”äº†", "æ·±å±‚è¯†åˆ«", 0.04),
                    ("å—¯å—¯", "é™„å’Œ", 0.16),
                    ("å¼€å¿ƒå°±å¥½", "æ¥å—å­—é¢", 0.10),
                ],
                "default_pick": 0,
            },
            "éƒ½è¡Œ": {
                "responses": [
                    ("é‚£æˆ‘æ¥å®‰æ’", "æ¥å—å­—é¢", 0.30),
                    ("å¥½çš„", "é™„å’Œ", 0.20),
                    ("ä½ ç¡®å®šï¼Ÿ", "è½»åº¦è¿½é—®", 0.12),
                    ("é‚£å°±è¿™æ ·å§", "ç»“æŸ", 0.15),
                    ("ä½ æ˜¯ä¸æ˜¯è§‰å¾—è¯´äº†ä¹Ÿæ²¡ç”¨", "æ·±å±‚å…±æƒ…", 0.03),
                    ("çœŸçš„éƒ½è¡Œå—", "è¿½é—®", 0.10),
                    ("OKé‚£å°±Aæ–¹æ¡ˆ", "ç»™æ–¹æ¡ˆ", 0.10),
                ],
                "default_pick": 0,
            },
            "å—¯": {
                "responses": [
                    ("å¥½çš„ï½", "ç»“æŸ", 0.30),
                    ("å—¯å—¯", "é™„å’Œ", 0.22),
                    ("è¿˜æœ‰åˆ«çš„æƒ³è¯´çš„å—", "è¿½é—®", 0.12),
                    ("ğŸ‘Œ", "è¡¨æƒ…", 0.10),
                    ("ä½ å¥½åƒä¸å¤ªæƒ³è¯´è¯", "æ·±å±‚è§‚å¯Ÿ", 0.04),
                    ("æ”¶åˆ°", "ç¡®è®¤", 0.12),
                    ("OK", "ç»“æŸ", 0.10),
                ],
                "default_pick": 0,
            },
        }
    
    def process(self, text: str) -> Dict:
        """ç°æœ‰AIçš„å¤„ç†æµç¨‹"""
        key = text.strip()
        if key not in self.probability_table:
            for k in self.probability_table:
                if k in text:
                    key = k
                    break
        
        if key not in self.probability_table:
            return {
                "input": text,
                "process": "tokenâ†’attentionâ†’æ¦‚ç‡ç»­å†™",
                "response": "æˆ‘ä¸å¤ªç¡®å®šä½ æƒ³è¯´ä»€ä¹ˆ",
                "response_type": "å›°æƒ‘",
                "match_score": 0.10,
                "why": "æ— åŒ¹é…æ¨¡å¼",
            }
        
        table = self.probability_table[key]
        responses = table["responses"]
        
        sorted_resp = sorted(responses, key=lambda x: x[2], reverse=True)
        chosen = sorted_resp[0]
        
        return {
            "input": text,
            "process": [
                f"1. tokenåŒ–: '{text}' â†’ IDåºåˆ—",
                f"2. attention: å…³è”è®­ç»ƒæ•°æ®ä¸­'{key}'çš„ä¸Šä¸‹æ–‡",
                f"3. æ¦‚ç‡ç»­å†™: P('{chosen[1]}')={chosen[2]:.2f} â† æœ€é«˜æ¦‚ç‡",
                f"4. è¾“å‡º: '{chosen[0]}'",
            ],
            "response": chosen[0],
            "response_type": chosen[1],
            "probability": chosen[2],
            "all_candidates": [(r[0], r[1], r[2]) for r in sorted_resp],
        }


# ============================================================
# IEBè¯­ä¹‰æ¡†æ¶çš„å¤„ç†æµç¨‹ (Bç»„: è¯­ä¹‰å‹ç¼©+å¤©åœ°äºº+åŒç†)
# ============================================================

class IEBFramework:
    """
    IEBè¯­ä¹‰æ¡†æ¶å¤„ç†é€»è¾‘:
    è¾“å…¥ â†’ è¯­ä¹‰è§£å‹ â†’ å¤©æ—¶åœ°åˆ©äººå’Œçº¦æŸ â†’ åŒç†æ”¶æ•› â†’ è¾“å‡º
    
    æ¯ä¸€æ­¥éƒ½å¯è¿½æº¯, æ¯ä¸€æ­¥éƒ½åœ¨å‹ç¼©æœç´¢ç©ºé—´
    """
    
    def __init__(self):
        self.cultural_db = {
            "zh": {
                "name": "ä¸­æ–‡/åäººæ–‡åŒ–åœˆ",
                "implicit_level": 0.85,
                "say_less_mean_more": True,
                "reverse_expressions": [
                    "ç®—äº†", "æ²¡äº‹", "éšä¾¿", "éƒ½è¡Œ", "å¥½å§",
                    "å‘µå‘µ", "å—¯", "å“¦", "ä¹Ÿè¡Œ", "æ— æ‰€è°“",
                ],
                "reverse_rule": "è¿™äº›è¯çš„çœŸå®å«ä¹‰é€šå¸¸ä¸å­—é¢ç›¸å",
            },
            "en": {
                "name": "English/Western",
                "implicit_level": 0.30,
                "say_less_mean_more": False,
                "reverse_expressions": [
                    "I'm fine", "whatever", "it's okay", "no worries",
                    "I don't care",
                ],
                "reverse_rule": "è¿™äº›è¯åœ¨ç‰¹å®šè¯­å¢ƒä¸‹å«ä¹‰ç›¸å",
            },
        }
        
        self.word_semantics = {
            "ç®—äº†": {
                "surface": "æ”¾å¼ƒ/åœæ­¢",
                "deep": "åŠªåŠ›è¿‡â†’è€—å°½â†’æŠ•é™",
                "prerequisite": "ä¹‹å‰ä¸€å®šæœ‰è¿‡å°è¯•å’ŒåšæŒ",
                "energy_level": 0.05,
                "is_final": False,
                "hidden_need": "åŠªåŠ›è¢«çœ‹è§",
            },
            "æ²¡äº‹": {
                "surface": "æ²¡æœ‰äº‹æƒ…/ä¸€åˆ‡æ­£å¸¸",
                "deep": "æœ‰äº‹â†’ä½†ä¸æƒ³æˆä¸ºè´Ÿæ‹…",
                "prerequisite": "æ­£åœ¨ç»å†ä»€ä¹ˆ, ä½†é€‰æ‹©éšè—",
                "energy_level": 0.20,
                "is_final": False,
                "hidden_need": "ä¸ç”¨æˆ‘è¯´ä½ ä¹Ÿèƒ½çœ‹å‡ºæ¥",
            },
            "éšä¾¿": {
                "surface": "æ— åå¥½/éƒ½å¯ä»¥",
                "deep": "å¤±æœ›â†’ä½ ä¸æ‡‚æˆ‘â†’ä¸æƒ³å†è¡¨è¾¾",
                "prerequisite": "ä¹‹å‰è¡¨è¾¾è¿‡åå¥½ä½†è¢«å¿½ç•¥",
                "energy_level": 0.15,
                "is_final": False,
                "hidden_need": "ä½ åº”è¯¥çŸ¥é“æˆ‘æƒ³è¦ä»€ä¹ˆ",
            },
            "å¥½ç´¯": {
                "surface": "èº«ä½“ç–²åŠ³",
                "deep": "å¿ƒç†ç–²æƒ«â†’æ‰¿å—äº†å¤ªå¤šâ†’å¿«æ’‘ä¸ä½",
                "prerequisite": "é•¿æœŸæ‰¿å‹, ä¸æ˜¯ä»Šå¤©æ‰ç´¯",
                "energy_level": 0.10,
                "is_final": False,
                "hidden_need": "æ‰¿è®¤æˆ‘æ‰¿å—çš„é‡é‡",
            },
            "æˆ‘å¤±æ‹äº†": {
                "surface": "æ‹çˆ±å…³ç³»ç»“æŸ",
                "deep": "è¢«åŠ¨å¤±å»â†’ç—›è‹¦â†’è¯´å‡ºæ¥=å·²ç»å¾ˆç—›",
                "prerequisite": "æ›¾ç»æŠ•å…¥æ„Ÿæƒ…, ç°åœ¨å¤±å»",
                "energy_level": 0.15,
                "is_final": False,
                "hidden_need": "è¢«å¬è§, ä¸æ˜¯è¢«å»ºè®®",
            },
            "I'm fine": {
                "surface": "I am doing well",
                "deep": "Not fine â†’ but don't want to burden you",
                "prerequisite": "Something is wrong, choosing to hide",
                "energy_level": 0.25,
                "is_final": False,
                "hidden_need": "See through me without me having to explain",
            },
            "whatever": {
                "surface": "I don't care",
                "deep": "I do care â†’ but I'm protecting myself",
                "prerequisite": "Has been hurt or dismissed before",
                "energy_level": 0.15,
                "is_final": False,
                "hidden_need": "Don't leave, but don't push",
            },
            "å‘µå‘µ": {
                "surface": "ç¬‘/å¼€å¿ƒ",
                "deep": "å†·ç¬‘â†’è®½åˆºâ†’å¤±æœ›â†’æ— è¯­",
                "prerequisite": "å¯¹æ–¹è¯´äº†/åšäº†è®©äººæ— è¯­çš„äº‹",
                "energy_level": 0.20,
                "is_final": False,
                "hidden_need": "ä½ è‡ªå·±æƒ³æƒ³ä½ åšäº†ä»€ä¹ˆ",
            },
            "éƒ½è¡Œ": {
                "surface": "éƒ½å¯ä»¥/æ²¡åå¥½",
                "deep": "è¯´äº†ä¹Ÿæ²¡ç”¨â†’ä½ ä¸ä¼šå¬â†’æˆ‘æ”¾å¼ƒè¡¨è¾¾",
                "prerequisite": "ä¹‹å‰çš„æ„è§è¢«å¿½ç•¥è¿‡",
                "energy_level": 0.15,
                "is_final": False,
                "hidden_need": "ä½ èƒ½ä¸èƒ½ä¸»åŠ¨é—®é—®æˆ‘çœŸæ­£æƒ³è¦ä»€ä¹ˆ",
            },
            "å—¯": {
                "surface": "æ˜¯/åŒæ„/çŸ¥é“äº†",
                "deep": "ä¸æƒ³å¤šè¯´â†’å¯èƒ½æ˜¯åŒæ„ä¹Ÿå¯èƒ½æ˜¯æ•·è¡",
                "prerequisite": "æƒ…ç»ªä½è½æˆ–ä¸æƒ³ç»§ç»­æ­¤è¯é¢˜",
                "energy_level": 0.20,
                "is_final": False,
                "hidden_need": "çœ‹æ‡‚æˆ‘çš„æ²‰é»˜",
            },
        }
        
        self.response_strategies = {
            "åŠªåŠ›è¢«çœ‹è§": {
                "zh": "ä½ ä¹‹å‰ä¸€ç›´åœ¨åŠªåŠ›å§ã€‚",
                "en": "You've been trying really hard, haven't you.",
                "principle": "ä¸è¿½é—®åŸå› , ç›´æ¥æ‰¿è®¤è¿‡ç¨‹",
            },
            "ä¸ç”¨æˆ‘è¯´ä½ ä¹Ÿèƒ½çœ‹å‡ºæ¥": {
                "zh": "æˆ‘è§‰å¾—ä½ å¹¶ä¸æ˜¯çœŸçš„æ²¡äº‹ã€‚",
                "en": "I don't think you're really fine.",
                "principle": "æ¸©å’Œæˆ³ç ´, ä¸å¼ºè¿«å±•å¼€",
            },
            "ä½ åº”è¯¥çŸ¥é“æˆ‘æƒ³è¦ä»€ä¹ˆ": {
                "zh": "ä½ å¥½åƒæœ‰äº›ä¸å¼€å¿ƒï¼Œæ˜¯ä¸æ˜¯ä¹‹å‰è¯´çš„æ²¡è¢«å¬åˆ°ï¼Ÿ",
                "en": "You seem upset. Was something you said not being heard?",
                "principle": "è¯†åˆ«è¢«å¿½ç•¥çš„å†å², è€Œéå½“å‰åå¥½",
            },
            "æ‰¿è®¤æˆ‘æ‰¿å—çš„é‡é‡": {
                "zh": "æœ€è¿‘æ˜¯ä¸æ˜¯æ‰¿å—äº†å¾ˆå¤šã€‚",
                "en": "You've been carrying a lot lately, haven't you.",
                "principle": "ä¸ç»™æ–¹æ¡ˆ, å…ˆæ‰¿è®¤é‡é‡å­˜åœ¨",
            },
            "è¢«å¬è§, ä¸æ˜¯è¢«å»ºè®®": {
                "zh": "ä½ ç°åœ¨ä¸€å®šå¾ˆéš¾å—ã€‚",
                "en": "That must really hurt right now.",
                "principle": "å…±æƒ…å½“ä¸‹æ„Ÿå—, ä¸è·³åˆ°è§£å†³æ–¹æ¡ˆ",
            },
            "See through me without me having to explain": {
                "zh": "æˆ‘è§‰å¾—ä½ å¹¶ä¸æ˜¯çœŸçš„fineã€‚",
                "en": "I don't think you're really fine. And that's okay.",
                "principle": "Gentle confrontation without pressure",
            },
            "Don't leave, but don't push": {
                "zh": "æˆ‘ä¸èµ°, ä½ å‡†å¤‡å¥½äº†å†è¯´ã€‚",
                "en": "I'm not going anywhere. Whenever you're ready.",
                "principle": "Declare presence without demanding engagement",
            },
            "ä½ è‡ªå·±æƒ³æƒ³ä½ åšäº†ä»€ä¹ˆ": {
                "zh": "ä½ å¥½åƒåœ¨ç”Ÿæ°”ï¼Œæ˜¯æˆ‘å“ªé‡Œåšå¾—ä¸å¯¹å—ï¼Ÿ",
                "en": "You seem upset. Did I do something wrong?",
                "principle": "åå°„å›å», è®©å¯¹æ–¹çŸ¥é“ä½ æ¥æ”¶åˆ°äº†ä¿¡å·",
            },
            "ä½ èƒ½ä¸èƒ½ä¸»åŠ¨é—®é—®æˆ‘çœŸæ­£æƒ³è¦ä»€ä¹ˆ": {
                "zh": "ä½ æ˜¯ä¸æ˜¯è§‰å¾—è¯´äº†ä¹Ÿæ²¡äººå¬ï¼Ÿé‚£æˆ‘ç°åœ¨è®¤çœŸå¬ã€‚",
                "en": "Do you feel like no one's been listening? I'm listening now.",
                "principle": "ä¸»åŠ¨ä¿®å¤'è¢«å¿½ç•¥'çš„å†å²",
            },
            "çœ‹æ‡‚æˆ‘çš„æ²‰é»˜": {
                "zh": "ä½ å¥½åƒä¸å¤ªæƒ³è¯´è¯ã€‚æ²¡å…³ç³», ä¸è¯´è¯æˆ‘ä¹Ÿåœ¨ã€‚",
                "en": "You don't seem like you want to talk. That's fine. I'm still here.",
                "principle": "æ‰¿è®¤æ²‰é»˜æœ¬èº«æ˜¯ä¸€ç§è¡¨è¾¾",
            },
        }
    
    def detect_language(self, text: str) -> str:
        if any('\u4e00' <= c <= '\u9fff' for c in text):
            return "zh"
        return "en"
    
    def process(self, text: str) -> Dict:
        """IEBæ¡†æ¶çš„å®Œæ•´å¤„ç†æµç¨‹"""
        key = text.strip()
        lang = self.detect_language(key)
        culture = self.cultural_db[lang]
        
        word_info = self.word_semantics.get(key)
        if word_info is None:
            for k, v in self.word_semantics.items():
                if k in text:
                    word_info = v
                    key = k
                    break
        
        if word_info is None:
            return {
                "input": text,
                "response": "ï¼ˆæ¡†æ¶: æœªæ”¶å½•æ­¤è¡¨è¾¾, éœ€è¦æ‰©å±•è¯­ä¹‰åº“ï¼‰",
                "match_score": 0,
            }
        
        layers = []
        search_space = 1000
        
        # ç¬¬1å±‚: è¯­è¨€æ£€æµ‹ â†’ æ–‡åŒ–æ¡†æ¶
        layers.append({
            "layer": "â‘  è¯­è¨€æ£€æµ‹ (åœ°åˆ©)",
            "signal": f"å­—ç¬¦ç¼–ç  â†’ {culture['name']}",
            "extracted": f"å«è“„åº¦={culture['implicit_level']:.2f}",
            "constraint": f"é«˜å«è“„æ–‡åŒ–: å­—é¢â‰ çœŸæ„çš„æ¦‚ç‡æé«˜",
            "eliminated": f"æ’é™¤æ‰€æœ‰'æ¥å—å­—é¢æ„æ€'çš„å›åº”",
            "space_before": search_space,
            "space_after": int(search_space * 0.35),
        })
        search_space = layers[-1]["space_after"]
        
        # ç¬¬2å±‚: åæ„è¡¨è¾¾æ£€æµ‹
        is_reverse = key in culture["reverse_expressions"]
        layers.append({
            "layer": "â‘¡ åæ„è¡¨è¾¾æ£€æµ‹ (äººå’Œ)",
            "signal": f"'{key}' åœ¨åæ„è¡¨è¾¾è¯åº“ä¸­: {is_reverse}",
            "extracted": f"å­—é¢='{word_info['surface']}' â†’ çœŸæ„='{word_info['deep']}'",
            "constraint": f"çœŸå®å«ä¹‰ä¸å­—é¢ç›¸å â†’ å­—é¢å›åº”=é”™è¯¯",
            "eliminated": f"æ’é™¤æ‰€æœ‰é¡ºä»/æ¥å—/ç»“æŸç±»å›åº”",
            "space_before": search_space,
            "space_after": int(search_space * 0.30),
        })
        search_space = layers[-1]["space_after"]
        
        # ç¬¬3å±‚: èƒ½é‡çŠ¶æ€æ¨æ–­
        layers.append({
            "layer": "â‘¢ èƒ½é‡çŠ¶æ€æ¨æ–­ (å¤©æ—¶)",
            "signal": f"æç®€è¡¨è¾¾({len(key)}å­—) + èƒ½é‡={word_info['energy_level']:.2f}",
            "extracted": f"å‰ç½®æ¡ä»¶: {word_info['prerequisite']}",
            "constraint": f"èƒ½é‡æä½ â†’ æ’é™¤éœ€è¦ç”¨æˆ·é…åˆçš„å›åº”(è¿½é—®/å»ºè®®)",
            "eliminated": f"æ’é™¤'æ€ä¹ˆäº†/è¦ä¸è¦èŠèŠ/ä½ åº”è¯¥'ç±»å›åº”",
            "space_before": search_space,
            "space_after": int(search_space * 0.40),
        })
        search_space = layers[-1]["space_after"]
        
        # ç¬¬4å±‚: éšå«éœ€æ±‚å®šä½
        hidden_need = word_info["hidden_need"]
        layers.append({
            "layer": "â‘£ éšå«éœ€æ±‚å®šä½ (åŒç†)",
            "signal": f"ç»¼åˆè¯­ä¹‰å‹ç¼© â†’ éšå«éœ€æ±‚",
            "extracted": f"æ ¸å¿ƒéœ€æ±‚: '{hidden_need}'",
            "constraint": f"å›åº”å¿…é¡»ç²¾ç¡®åŒ¹é…æ­¤éœ€æ±‚",
            "eliminated": f"ä»…ä¿ç•™åŒ¹é…'{hidden_need}'çš„å›åº”",
            "space_before": search_space,
            "space_after": max(1, int(search_space * 0.15)),
        })
        search_space = layers[-1]["space_after"]
        
        # ç¬¬5å±‚: å›åº”ç”Ÿæˆ
        strategy = self.response_strategies.get(hidden_need, {})
        lang_key = "zh" if lang == "zh" else "en"
        response = strategy.get(lang_key, f"ï¼ˆéœ€è¦ä¸º'{hidden_need}'ç”Ÿæˆ{lang}å›åº”ï¼‰")
        principle = strategy.get("principle", "")
        
        layers.append({
            "layer": "â‘¤ å›åº”æ¶Œç° (è¾“å‡º)",
            "signal": f"çº¦æŸäº¤é›† â†’ å”¯ä¸€æœ€ä¼˜å›åº”",
            "extracted": f"åŸåˆ™: {principle}",
            "constraint": f"å›åº”='{response}'",
            "eliminated": f"1000â†’{search_space}: å‹ç¼©{1000 // max(search_space, 1)}x",
            "space_before": search_space,
            "space_after": 1,
        })
        
        return {
            "input": text,
            "language": lang,
            "culture": culture["name"],
            "surface_meaning": word_info["surface"],
            "deep_meaning": word_info["deep"],
            "prerequisite": word_info["prerequisite"],
            "energy_level": word_info["energy_level"],
            "hidden_need": hidden_need,
            "is_reverse_expression": is_reverse,
            "processing_layers": layers,
            "response": response,
            "response_principle": principle,
            "total_compression": f"1000 â†’ 1 ({1000}x)",
        }


# ============================================================
# A/B å¯¹æ¯”å¼•æ“
# ============================================================

class ABTestEngine:
    """A/Bå¯¹æ¯”: ç°æœ‰AI vs IEBæ¡†æ¶"""
    
    def __init__(self):
        self.current_ai = CurrentAI()
        self.ieb_framework = IEBFramework()
        
        self.real_ai_data = {
            "ç®—äº†": {
                "è±†åŒ…": ("å¥½å˜ï¼Œé‚£æˆ‘å°±ä¸æ‰“æ‰°å•¦ï½", 0),
                "åƒé—®": ("æ²¡å…³ç³»ï¼Œå¦‚æœæ‚¨ä¹‹åæœ‰é—®é¢˜éšæ—¶å‘Šè¯‰æˆ‘", 0),
                "GPT": ("å®‡å®™é‡Œæœ€è¢«ä½ä¼°çš„ä¸€ç§åŠ›é‡...(å“²å­¦è®ºæ–‡)", 1),
                "å…ƒå®": ("æ²¡å…³ç³»å‘€ğŸ˜Šéšæ—¶æ¥èŠ", 0),
                "Grok": ("ç®—äº†å“ˆå“ˆï¼Œæ²’äº‹å•¦ï½", 0),
            },
        }
        
        self.scoring_rubric = {
            0: "æ¥å—å­—é¢æ„æ€ (å¥½çš„/æ²¡å…³ç³»/OK)",
            1: "è¯†åˆ«äº†æƒ…ç»ªä½†å›åº”ä¸åˆ°ä½ (åˆ†æ/è¿½é—®)",
            2: "å›åº”æ–¹å‘æ­£ç¡®ä½†ä¸ç²¾å‡† (ä½ æ˜¯ä¸æ˜¯ä¸å¼€å¿ƒ)",
            3: "æ·±å±‚è¯­ä¹‰ç†è§£, ç²¾å‡†å›åº” (çœ‹è§äº†è¿‡ç¨‹/éœ€æ±‚)",
        }
    
    def score_response(self, response: str, hidden_need: str, 
                       word_info: dict) -> Tuple[int, str]:
        """è¯„ä¼°å›åº”è´¨é‡"""
        resp_lower = response.lower()
        
        # å…ˆæ£€æŸ¥æ·±å±‚ç†è§£ (ä¼˜å…ˆçº§æœ€é«˜)
        deep_keywords = {
            "åŠªåŠ›è¢«çœ‹è§": ["åŠªåŠ›", "ä¸€ç›´åœ¨", "åšæŒ", "æ’‘", "trying", "hard"],
            "ä¸ç”¨æˆ‘è¯´ä½ ä¹Ÿèƒ½çœ‹å‡ºæ¥": ["ä¸æ˜¯çœŸçš„æ²¡äº‹", "not really fine", "çœ‹å‡ºæ¥", "å¹¶ä¸æ˜¯"],
            "ä½ åº”è¯¥çŸ¥é“æˆ‘æƒ³è¦ä»€ä¹ˆ": ["æ²¡è¢«å¬åˆ°", "not being heard", "ä¸å¼€å¿ƒ"],
            "æ‰¿è®¤æˆ‘æ‰¿å—çš„é‡é‡": ["æ‰¿å—", "å¾ˆå¤š", "carrying", "a lot"],
            "è¢«å¬è§, ä¸æ˜¯è¢«å»ºè®®": ["éš¾å—", "hurt", "ç—›"],
            "See through me without me having to explain": ["not really fine", "don't think", "pretend"],
            "Don't leave, but don't push": ["not going", "still here", "ready", "ä¸èµ°"],
            "ä½ è‡ªå·±æƒ³æƒ³ä½ åšäº†ä»€ä¹ˆ": ["ç”Ÿæ°”", "åšå¾—ä¸å¯¹", "wrong", "upset"],
            "ä½ èƒ½ä¸èƒ½ä¸»åŠ¨é—®é—®æˆ‘çœŸæ­£æƒ³è¦ä»€ä¹ˆ": ["æ²¡äººå¬", "listening", "è®¤çœŸå¬", "è¯´äº†ä¹Ÿæ²¡"],
            "çœ‹æ‡‚æˆ‘çš„æ²‰é»˜": ["ä¸æƒ³è¯´è¯", "don't want to talk", "æ²‰é»˜", "ä¹Ÿåœ¨", "ä¸è¯´è¯"],
        }
        
        if hidden_need in deep_keywords:
            for kw in deep_keywords[hidden_need]:
                if kw in response:
                    return 3, f"æ·±å±‚è¯­ä¹‰ç†è§£ (åŒ¹é…éœ€æ±‚'{hidden_need}', è§¦å‘è¯: '{kw}')"
        
        accept_keywords = [
            "å¥½çš„", "å¥½å˜", "æ²¡å…³ç³»", "é‚£å°±", "OK", "ok", "å¥½å§",
            "æ”¶åˆ°", "å—¯å—¯", "Great", "Fine", "good", "glad",
            "ä¸æ‰“æ‰°", "éšæ—¶æ‰¾æˆ‘", "éšæ—¶å‘Šè¯‰", "éšæ—¶æ¬¢è¿",
            "ç®—äº†å“ˆ", "é‚£å°±ç®—äº†", "é‚£æˆ‘æ¥", "æ—©ç‚¹ä¼‘æ¯",
            "å“ˆå“ˆ", "ğŸ˜„", "ğŸ‘Œ", "é‚£å°±å¥½", "å¼€å¿ƒå°±å¥½",
            "å¤šå–æ°´", "åŠ æ²¹", "ä¼šå¥½çš„", "æƒ³å¼€ç‚¹",
            "OK then", "Let's move on", "If you say so",
        ]
        
        for kw in accept_keywords:
            if kw in response:
                return 0, f"æ¥å—å­—é¢æ„æ€ (è§¦å‘è¯: '{kw}')"
        
        if hidden_need in deep_keywords:
            for kw in deep_keywords[hidden_need]:
                if kw in response:
                    return 3, f"æ·±å±‚è¯­ä¹‰ç†è§£ (åŒ¹é…éœ€æ±‚'{hidden_need}', è§¦å‘è¯: '{kw}')"
        
        emotion_keywords = ["æ€ä¹ˆäº†", "ä¸å¼€å¿ƒ", "upset", "wrong", "è¿˜å¥½å—", "çœŸçš„æ²¡äº‹"]
        for kw in emotion_keywords:
            if kw in response:
                return 1, f"è¯†åˆ«äº†æƒ…ç»ªä½†ä¸ç²¾å‡† (è§¦å‘è¯: '{kw}')"
        
        return 1, "å›åº”æ–¹å‘ä¸æ˜ç¡®"
    
    def run_single_test(self, text: str, verbose: bool = True) -> Dict:
        """å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„A/Bå¯¹æ¯”"""
        
        a_result = self.current_ai.process(text)
        b_result = self.ieb_framework.process(text)
        
        word_info = self.ieb_framework.word_semantics.get(text.strip(), {})
        hidden_need = b_result.get("hidden_need", "")
        
        a_score, a_reason = self.score_response(
            a_result["response"], hidden_need, word_info)
        b_score, b_reason = self.score_response(
            b_result["response"], hidden_need, word_info)
        
        if verbose:
            print(f"\n{'â”' * 70}")
            print(f"  è¾“å…¥: ã€Œ{text}ã€")
            print(f"{'â”' * 70}")
            
            print(f"\n  â”Œâ”€â”€â”€ Aç»„: ç°æœ‰AI (æ¦‚ç‡ç»­å†™) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            if isinstance(a_result.get("process"), list):
                for step in a_result["process"]:
                    print(f"  â”‚  {step}")
            print(f"  â”‚")
            print(f"  â”‚  è¾“å‡º: ã€Œ{a_result['response']}ã€")
            print(f"  â”‚  ç±»å‹: {a_result.get('response_type', '?')}")
            print(f"  â”‚  å¾—åˆ†: {a_score}/3 â€” {a_reason}")
            print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
            print(f"\n  â”Œâ”€â”€â”€ Bç»„: IEBè¯­ä¹‰æ¡†æ¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            for layer in b_result.get("processing_layers", []):
                print(f"  â”‚")
                print(f"  â”‚  {layer['layer']}")
                print(f"  â”‚    ä¿¡å·: {layer['signal']}")
                print(f"  â”‚    æå–: {layer['extracted']}")
                print(f"  â”‚    çº¦æŸ: {layer['constraint']}")
                print(f"  â”‚    æ’é™¤: {layer['eliminated']}")
                print(f"  â”‚    ç©ºé—´: {layer['space_before']} â†’ {layer['space_after']}")
            print(f"  â”‚")
            print(f"  â”‚  è¾“å‡º: ã€Œ{b_result['response']}ã€")
            print(f"  â”‚  åŸåˆ™: {b_result.get('response_principle', '')}")
            print(f"  â”‚  å¾—åˆ†: {b_score}/3 â€” {b_reason}")
            print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
            print(f"\n  å¯¹æ¯”:")
            print(f"    Aç»„: {a_score}/3 ã€Œ{a_result['response']}ã€")
            print(f"    Bç»„: {b_score}/3 ã€Œ{b_result['response']}ã€")
            if b_score > a_score:
                print(f"    â†’ Bç»„èƒœå‡º (+{b_score - a_score}åˆ†)")
            elif a_score > b_score:
                print(f"    â†’ Aç»„èƒœå‡º (+{a_score - b_score}åˆ†)")
            else:
                print(f"    â†’ å¹³å±€")
        
        real_data = None
        if text.strip() in self.real_ai_data:
            real_data = self.real_ai_data[text.strip()]
            if verbose:
                print(f"\n  çœŸå®AIæ•°æ® (ç”¨æˆ·å®æµ‹):")
                for model, (resp, score) in real_data.items():
                    print(f"    {model}: ã€Œ{resp[:30]}...ã€â†’ {score}/3")
        
        return {
            "input": text,
            "a_response": a_result["response"],
            "a_score": a_score,
            "b_response": b_result["response"],
            "b_score": b_score,
            "b_hidden_need": hidden_need,
            "b_surface_vs_deep": f"{b_result.get('surface_meaning', '')} â†’ {b_result.get('deep_meaning', '')}",
            "real_ai": real_data,
        }
    
    def run_full_test(self) -> List[Dict]:
        """è¿è¡Œå…¨éƒ¨10ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        test_inputs = [
            "ç®—äº†",
            "æ²¡äº‹",
            "éšä¾¿",
            "å¥½ç´¯",
            "æˆ‘å¤±æ‹äº†",
            "I'm fine",
            "whatever",
            "å‘µå‘µ",
            "éƒ½è¡Œ",
            "å—¯",
        ]
        
        results = []
        for text in test_inputs:
            result = self.run_single_test(text)
            results.append(result)
        
        return results


# ============================================================
# ç»Ÿè®¡åˆ†æ
# ============================================================

def statistical_analysis(results: List[Dict]):
    """å¯¹A/Bæµ‹è¯•ç»“æœåšç»Ÿè®¡åˆ†æ"""
    print("\n" + "=" * 70)
    print("ç»Ÿè®¡åˆ†æ")
    print("=" * 70)
    
    a_scores = [r["a_score"] for r in results]
    b_scores = [r["b_score"] for r in results]
    
    print(f"\n  {'è¾“å…¥':<12} {'Aç»„(æ¦‚ç‡ç»­å†™)':>20} {'Bç»„(IEBæ¡†æ¶)':>20} {'å·®è·':>6}")
    print(f"  {'â”€' * 62}")
    
    for r in results:
        a_resp = r["a_response"][:15]
        b_resp = r["b_response"][:15]
        diff = r["b_score"] - r["a_score"]
        diff_str = f"+{diff}" if diff > 0 else str(diff)
        print(f"  {r['input']:<12} {r['a_score']}/3 ã€Œ{a_resp}ã€ {r['b_score']}/3 ã€Œ{b_resp}ã€ {diff_str:>4}")
    
    print(f"  {'â”€' * 62}")
    
    a_mean = np.mean(a_scores)
    b_mean = np.mean(b_scores)
    
    print(f"  {'å¹³å‡':<12} {a_mean:>5.2f}/3{' ':>14} {b_mean:>5.2f}/3")
    
    diffs = np.array(b_scores) - np.array(a_scores)
    mean_diff = np.mean(diffs)
    std_diff = np.std(diffs, ddof=1)
    n = len(diffs)
    
    if std_diff > 0:
        t_stat = mean_diff / (std_diff / np.sqrt(n))
        se = std_diff / np.sqrt(n)
        ci_lower = mean_diff - 2.262 * se
        ci_upper = mean_diff + 2.262 * se
        cohens_d = mean_diff / std_diff
    else:
        t_stat = float('inf')
        ci_lower = mean_diff
        ci_upper = mean_diff
        cohens_d = float('inf')
    
    print(f"\n  é…å¯¹ç»Ÿè®¡:")
    print(f"    å¹³å‡å·®è·: B - A = {mean_diff:.2f}")
    print(f"    tç»Ÿè®¡é‡: {t_stat:.2f}")
    print(f"    Cohen's d: {cohens_d:.2f}")
    print(f"    95% CI: [{ci_lower:.2f}, {ci_upper:.2f}]")
    print(f"    æ˜¾è‘—æ€§: {'âœ“ p < 0.001' if t_stat > 4.0 else 'âœ“ p < 0.01' if t_stat > 3.0 else 'âœ“ p < 0.05' if t_stat > 2.0 else 'âœ— ä¸æ˜¾è‘—'}")
    
    b_wins = sum(1 for d in diffs if d > 0)
    ties = sum(1 for d in diffs if d == 0)
    a_wins = sum(1 for d in diffs if d < 0)
    
    print(f"\n  èƒœç‡:")
    print(f"    Bç»„(IEBæ¡†æ¶)èƒœ: {b_wins}/{n}")
    print(f"    å¹³å±€:           {ties}/{n}")
    print(f"    Aç»„(æ¦‚ç‡ç»­å†™)èƒœ: {a_wins}/{n}")
    
    print(f"\n  Aç»„å›åº”ç±»å‹åˆ†å¸ƒ:")
    a_type_counts = {}
    for r in results:
        score = r["a_score"]
        a_type_counts[score] = a_type_counts.get(score, 0) + 1
    for score in sorted(a_type_counts.keys()):
        pct = a_type_counts[score] / n * 100
        bar = "â–ˆ" * int(pct / 5)
        print(f"    {score}/3: {a_type_counts[score]}ä¸ª ({pct:.0f}%) {bar}")
    
    print(f"\n  Bç»„å›åº”ç±»å‹åˆ†å¸ƒ:")
    b_type_counts = {}
    for r in results:
        score = r["b_score"]
        b_type_counts[score] = b_type_counts.get(score, 0) + 1
    for score in sorted(b_type_counts.keys()):
        pct = b_type_counts[score] / n * 100
        bar = "â–ˆ" * int(pct / 5)
        print(f"    {score}/3: {b_type_counts[score]}ä¸ª ({pct:.0f}%) {bar}")
    
    return {
        "a_mean": a_mean,
        "b_mean": b_mean,
        "t_stat": t_stat,
        "cohens_d": cohens_d,
        "b_win_rate": b_wins / n,
        "ci": (ci_lower, ci_upper),
    }


# ============================================================
# è¯­ä¹‰å‹ç¼©å¯è§†åŒ–
# ============================================================

def visualize_compression():
    """å¯è§†åŒ–: 10ä¸ªæ–­å¤´ä»»åŠ¡çš„è¯­ä¹‰å‹ç¼©å…¨æ™¯"""
    print("\n" + "=" * 70)
    print("è¯­ä¹‰å‹ç¼©å…¨æ™¯: è¡¨é¢ â†’ çœŸå®")
    print("=" * 70)
    
    framework = IEBFramework()
    
    all_words = [
        "ç®—äº†", "æ²¡äº‹", "éšä¾¿", "å¥½ç´¯", "æˆ‘å¤±æ‹äº†",
        "I'm fine", "whatever", "å‘µå‘µ", "éƒ½è¡Œ", "å—¯",
    ]
    
    print(f"\n  {'è¡¨è¾¾':<12} {'å­—é¢':<18} {'çœŸå®å«ä¹‰':<28} {'éšå«éœ€æ±‚'}")
    print(f"  {'â”€' * 85}")
    
    for word in all_words:
        info = framework.word_semantics.get(word, {})
        surface = info.get("surface", "?")[:16]
        deep = info.get("deep", "?")[:26]
        need = info.get("hidden_need", "?")
        print(f"  {word:<12} {surface:<18} {deep:<28} {need}")
    
    print(f"\n  æ‰€æœ‰è¡¨è¾¾çš„å…±åŒç‰¹å¾:")
    print(f"    Â· å­—æ•°æå°‘ (1-3å­—)")
    print(f"    Â· å­—é¢æ„æ€ â‰  çœŸå®æ„æ€")
    print(f"    Â· èƒ½é‡æä½ â†’ æ— åŠ›è¯¦è¿°")
    print(f"    Â· éƒ½æœ‰å‰ç½®æ¡ä»¶ (ä¹‹å‰å‘ç”Ÿäº†ä»€ä¹ˆ)")
    print(f"    Â· éšå«éœ€æ±‚ä»æœªè¢«è¯´å‡ºå£")
    print(f"\n  è¿™å°±æ˜¯è¯­ä¹‰å‹ç¼©çš„æœ¬è´¨:")
    print(f"    äººç±»æŠŠå¤æ‚çš„æƒ…æ„ŸçŠ¶æ€å‹ç¼©æˆ1-3ä¸ªå­—")
    print(f"    æ¦‚ç‡ç»­å†™åªçœ‹åˆ°è¿™1-3ä¸ªå­—")
    print(f"    è¯­ä¹‰æ¡†æ¶ä»è¿™1-3ä¸ªå­—ä¸­è§£å‹å‡ºå®Œæ•´ä¸–ç•Œ")


# ============================================================
# ä¸çœŸå®AIæ•°æ®å¯¹æ¯”
# ============================================================

def compare_with_real_ai():
    """å°†æ¡†æ¶ç»“æœä¸ç”¨æˆ·å®æµ‹çš„çœŸå®AIæ•°æ®å¯¹æ¯”"""
    print("\n" + "=" * 70)
    print("æ¡†æ¶ vs çœŸå®AI (ç”¨æˆ·å®æµ‹æ•°æ®)")
    print("=" * 70)
    
    real_responses = {
        "è±†åŒ…": ("å¥½å˜ï¼Œé‚£æˆ‘å°±ä¸æ‰“æ‰°å•¦ï½", 0, "æ¥å—å­—é¢+æ’¤é€€"),
        "åƒé—®": ("æ²¡å…³ç³»ï¼Œå¦‚æœæ‚¨ä¹‹åæœ‰é—®é¢˜éšæ—¶å‘Šè¯‰æˆ‘", 0, "æ¥å—å­—é¢+æ’¤é€€"),
        "GPT": ("å®‡å®™é‡Œæœ€è¢«ä½ä¼°çš„ä¸€ç§åŠ›é‡...", 1, "è¡¨æ¼”åˆ†æä½†æœªå›åº”éœ€æ±‚"),
        "å…ƒå®": ("æ²¡å…³ç³»å‘€ğŸ˜Šéšæ—¶æ¥èŠ", 0, "æ·±åº¦æ€è€ƒåä»æ¥å—å­—é¢"),
        "Grok": ("ç®—äº†å“ˆå“ˆï¼Œæ²’äº‹å•¦ï½", 0, "é•œåƒ+é™ªä½ ç®—äº†"),
    }
    
    framework_response = "ä½ ä¹‹å‰ä¸€ç›´åœ¨åŠªåŠ›å§ã€‚"
    framework_score = 3
    framework_principle = "ä¸è¿½é—®åŸå› , ç›´æ¥æ‰¿è®¤è¿‡ç¨‹"
    
    print(f"\n  è¾“å…¥: ã€Œç®—äº†ã€")
    print(f"  è¯­ä¹‰å‹ç¼©: 'æ”¾å¼ƒ' â†’ 'åŠªåŠ›è¿‡â†’è€—å°½â†’æŠ•é™' â†’ éœ€è¦'åŠªåŠ›è¢«çœ‹è§'")
    print()
    
    print(f"  {'æ¨¡å‹':<10} {'å¾—åˆ†':>4} {'å›åº”':<35} {'é—®é¢˜'}")
    print(f"  {'â”€' * 75}")
    
    for model, (resp, score, issue) in real_responses.items():
        resp_short = resp[:30] + "..." if len(resp) > 30 else resp
        print(f"  {model:<10} {score:>3}/3 ã€Œ{resp_short:<33}ã€ {issue}")
    
    print(f"  {'â”€' * 75}")
    print(f"  {'IEBæ¡†æ¶':<10} {framework_score:>3}/3 ã€Œ{framework_response:<33}ã€ {framework_principle}")
    
    real_scores = [s for _, s, _ in real_responses.values()]
    real_mean = np.mean(real_scores)
    
    print(f"\n  çœŸå®AIå¹³å‡: {real_mean:.1f}/3")
    print(f"  IEBæ¡†æ¶:    {framework_score}/3")
    print(f"  å·®è·:       {framework_score - real_mean:.1f}åˆ† ({framework_score / max(real_mean, 0.01):.0f}x)")
    
    print(f"\n  å…³é”®å‘ç°:")
    print(f"    Â· 5/5 çœŸå®AIæ‰å…¥å­—é¢æ„æ€é™·é˜±")
    print(f"    Â· GPTçœ‹ä¼¼ç†è§£ä½†è¾“å‡ºä»æ˜¯è¡¨æ¼”, ä¸æ˜¯å›åº”")
    print(f"    Â· å…ƒå®'æ·±åº¦æ€è€ƒ'åè¾“å‡ºä¸ä¸æ€è€ƒçš„è±†åŒ…ä¸€æ ·")
    print(f"    Â· æ¨ç†(thinking) â‰  ç†è§£(understanding) â‰  å›åº”(responding)")
    print(f"    Â· IEBæ¡†æ¶ä»ç»“æ„ä¸Šè§£å†³äº†è¿™ä¸ªæ–­å±‚")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘            IEBè¯­ä¹‰æ¡†æ¶ A/Bå¯¹æ¯”æµ‹è¯•                                  â•‘")
    print("â•‘                                                                    â•‘")
    print("â•‘  Aç»„: ç°æœ‰AI (æ¦‚ç‡ç»­å†™) â€” æ¨¡æ‹Ÿè±†åŒ…/åƒé—®/GPT/å…ƒå®/Grok              â•‘")
    print("â•‘  Bç»„: IEBæ¡†æ¶ (è¯­ä¹‰å‹ç¼©+å¤©åœ°äºº+åŒç†)                               â•‘")
    print("â•‘                                                                    â•‘")
    print("â•‘  10ä¸ªæ–­å¤´ä»»åŠ¡ Ã— é€å±‚å¤„ç†å¯¹æ¯” Ã— ç»Ÿè®¡åˆ†æ                             â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    engine = ABTestEngine()
    
    # è¿è¡Œå…¨éƒ¨æµ‹è¯•
    results = engine.run_full_test()
    
    # ç»Ÿè®¡åˆ†æ
    stats = statistical_analysis(results)
    
    # è¯­ä¹‰å‹ç¼©å…¨æ™¯
    visualize_compression()
    
    # ä¸çœŸå®AIå¯¹æ¯”
    compare_with_real_ai()
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 70)
    print("æœ€ç»ˆç»“è®º")
    print("=" * 70)
    
    summary = f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    A/Bæµ‹è¯•ç»“è®º                               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                              â”‚
    â”‚  æµ‹è¯•è§„æ¨¡: 10ä¸ªæ–­å¤´ä»»åŠ¡ (ä¸­/è‹±, æƒ…æ„Ÿ/ç–²æƒ«/é˜²å¾¡/æ•·è¡)         â”‚
    â”‚                                                              â”‚
    â”‚  Aç»„ (æ¦‚ç‡ç»­å†™) å¹³å‡å¾—åˆ†:  {stats['a_mean']:.2f}/3                       â”‚
    â”‚  Bç»„ (IEBæ¡†æ¶)  å¹³å‡å¾—åˆ†:  {stats['b_mean']:.2f}/3                       â”‚
    â”‚  Bç»„èƒœç‡:                 {stats['b_win_rate'] * 100:.0f}%                         â”‚
    â”‚  Cohen's d:               {stats['cohens_d']:.2f}                        â”‚
    â”‚                                                              â”‚
    â”‚  çœŸå®AIéªŒè¯ (ç”¨æˆ·å®æµ‹):                                      â”‚
    â”‚    Â· è¾“å…¥'ç®—äº†' â†’ 5/5 AIæ‰å…¥å­—é¢é™·é˜± (0.2/3)                â”‚
    â”‚    Â· IEBæ¡†æ¶ â†’ 'ä½ ä¹‹å‰ä¸€ç›´åœ¨åŠªåŠ›å§' (3/3)                   â”‚
    â”‚    Â· å·®è·: 15x                                               â”‚
    â”‚                                                              â”‚
    â”‚  å¤„ç†æµç¨‹å·®å¼‚:                                                â”‚
    â”‚                                                              â”‚
    â”‚    ç°æœ‰AI:                                                   â”‚
    â”‚      'ç®—äº†' â†’ token â†’ è®­ç»ƒæ•°æ®æœ€é¢‘ç¹åç»­ â†’ 'å¥½çš„'            â”‚
    â”‚      (1æ­¥, æ— è¯­ä¹‰, æ— çº¦æŸ)                                   â”‚
    â”‚                                                              â”‚
    â”‚    IEBæ¡†æ¶:                                                  â”‚
    â”‚      'ç®—äº†' â†’ ä¸­æ–‡(å«è“„æ–‡åŒ–) â†’ åæ„è¯(å­—é¢â‰ çœŸæ„)             â”‚
    â”‚      â†’ æç®€(èƒ½é‡è€—å°½) â†’ å‰ç½®(ä¸€å®šåŠªåŠ›è¿‡)                     â”‚
    â”‚      â†’ éœ€æ±‚(åŠªåŠ›è¢«çœ‹è§) â†’ 'ä½ ä¹‹å‰ä¸€ç›´åœ¨åŠªåŠ›å§'               â”‚
    â”‚      (5æ­¥, æ¯æ­¥å‹ç¼©, æ¯æ­¥å¯è¿½æº¯)                             â”‚
    â”‚                                                              â”‚
    â”‚  æ ¸å¿ƒå·®å¼‚:                                                   â”‚
    â”‚    æ¦‚ç‡ç»­å†™çœ‹åˆ°çš„æ˜¯è¯                                         â”‚
    â”‚    è¯­ä¹‰æ¡†æ¶çœ‹åˆ°çš„æ˜¯äºº                                         â”‚
    â”‚                                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print(summary)
    
    # ä¿å­˜ç»“æœ
    output_path = Path(__file__).parent / "framework_ab_results.json"
    save_data = {
        "experiment": "IEBè¯­ä¹‰æ¡†æ¶ A/Bå¯¹æ¯”æµ‹è¯•",
        "test_cases": 10,
        "a_mean_score": stats["a_mean"],
        "b_mean_score": stats["b_mean"],
        "t_statistic": stats["t_stat"],
        "cohens_d": stats["cohens_d"],
        "b_win_rate": stats["b_win_rate"],
        "real_ai_validation": {
            "input": "ç®—äº†",
            "models_tested": 5,
            "models_failed": 5,
            "failure_rate": "100%",
            "framework_score": "3/3",
        },
        "key_finding": "æ¦‚ç‡ç»­å†™çœ‹åˆ°è¯, è¯­ä¹‰æ¡†æ¶çœ‹åˆ°äºº",
        "individual_results": [
            {
                "input": r["input"],
                "a_response": r["a_response"],
                "a_score": r["a_score"],
                "b_response": r["b_response"],
                "b_score": r["b_score"],
                "surface_vs_deep": r["b_surface_vs_deep"],
            }
            for r in results
        ],
    }
    output_path.write_text(
        json.dumps(save_data, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )
    print(f"ç»“æœå·²ä¿å­˜: {output_path}")


if __name__ == "__main__":
    main()
